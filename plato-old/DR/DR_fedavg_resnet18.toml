[clients]

# Type of client configuration
type = "simple"

# The total number of clients in the system
total_clients = 3

# The number of clients selected in each communication round
per_round = 3

# Should the clients compute test accuracy locally?
do_test = true 

# Whether client-server communication should be simulated with files
comm_simulation = true

# --- Client Heterogeneity Simulation Settings ---
# Enable simulation of client training speed differences
speed_simulation = true 
# Simulate sleep times (faster, more reproducible)
sleep_simulation = true 

# Max sleep time for the Pareto distribution
max_sleep_time = 60 

# Average training time (required if sleep_simulation is true)
avg_training_time = 3.0 

[clients.simulation_distribution]
# Parameters for simulating client heterogeneity in training speed (Pareto for stragglers)
distribution = "pareto"
alpha = 1 

[server]
address = "127.0.0.1"
port = 8000



[data]

# The training and testing dataset
datasource = "DRDataSource" # Required for custom image datasets like DR
dataset_name = "DR" # Dataset identifier
download = false # Assuming custom DR dataset is locally available
num_classes = 5

# --- Custom Data Source Parameters ---
# IMPORTANT: Replace this with your local path to the DR data directory
full_data_path = "/home/sushi_roll/my_project/plato-old/DR/"

# Name of the CSV file containing image IDs and diagnosis
labels_filename = "trainLabels.csv" 

# Subdirectory containing all the actual .png images
train_dir = "colored_images"
test_dir = "colored_images" # Uses the same image folder for the global test set

# Ratio of the full data to hold out for the global test set (server evaluation)
test_ratio = 0.50

random_seeds = [1, 2, 3]

# Full data path: You must replace this with your local path to the DR data directory
# full_data_path = "/path/to/your/dr_dataset_root/"

# Number of samples in each partition (Total dataset size)
partition_size = 500

# IID or non-IID?
sampler = "iid"

[trainer]

# Using the basic trainer
type = "DR.dr_trainer.DRTrainer"
#"basic"

# The maximum number of training rounds
rounds = 1

# The maximum number of clients running concurrently
max_concurrency = 1

# The target accuracy
target_accuracy = 0.85

# The machine learning model
model_name = "resnet_18"

# Number of epoches for local training in each communication round
epochs = 1
batch_size = 4
optimizer = "SGD"
lr_scheduler = "LambdaLR"

[algorithm]

# Aggregation algorithm
type = "DR.dr_fedavg.DRFedAvg"
#"fedavg"

[parameters]

[parameters.optimizer]
lr = 0.01
momentum = 0.9
weight_decay = 0.0

[parameters.learning_rate]
gamma = 0.1
milestone_steps = "80ep,120ep"

#[system] 
# Global random seeds for running the entire experiment multiple times for reproducibility

[results]
# Write the following parameter(s) into a CSV
#Run this line below if include_fields doesn't work!
#types = "round, elapsed_time, accuracy, comm_overhead, comm_time, processing_time, round_time, comm_overhead, local_epoch_num, edge_agg_num"
include_fields = ["round", "elapsed_time", "accuracy", "loss", "f1_macro", "recall_macro", "precision_macro", "comm_overhead", "round_time", "local_epoch_num", "edge_agg_num"]